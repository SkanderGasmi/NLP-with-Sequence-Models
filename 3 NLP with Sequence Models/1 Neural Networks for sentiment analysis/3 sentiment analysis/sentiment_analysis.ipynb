{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "859db0f1",
   "metadata": {
    "colab_type": "text",
    "id": "eM2WI45DjCVp"
   },
   "source": [
    "# Sentiment with Deep Neural Networks\n",
    "\n",
    "In this notebook, we will explore sentiment analysis using deep neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ea8e7",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Import Libraries and try out Trax](#1)\n",
    "- [2 - Importing the Data](#2)\n",
    "    - [2.1 - Loading in the Data](#2-1)\n",
    "    - [2.2 - Building the Vocabulary](#2-2)\n",
    "    - [2.3 - Converting a Tweet to a Tensor](#2-3)\n",
    "        - [2.3.1 - tweet_to_tensor ](#2.3.1)\n",
    "    - [2.4 - Creating a Batch Generator](#2-4)\n",
    "        - [2.4.1 - data_generator](#2.4.1)\n",
    "- [3 - Defining Classes](#3)\n",
    "    - [3.1 - ReLU Class](#3-1)\n",
    "        - [3.2.1 - Relu](#3.1.1)\n",
    "    - [3.2 - Dense Class](#3.2)\n",
    "        - [3.3.1 - Dense](#3.3.1)\n",
    "    - [3.3 - Model](#3-3)\n",
    "        - [3.3.1 classifier](#3.3.1)\n",
    "- [4 - Training](#4)\n",
    "    - [4.1 Training the Model](#4-1)\n",
    "        - [4.1.1 - train_model](#4.1.1)\n",
    "    - [4.2 - Making a Prediction](#4-2)\n",
    "- [5 - Evaluation](#5)\n",
    "    - [5.1 - Computing the Accuracy on a Batch](#5-1)\n",
    "        - [5.1.1 - compute_accuracy](#5.1.1)\n",
    "    - [5.2 - Testing your Model on Validation Data](#5-2)\n",
    "        - [5.2.1 - test_model](#5.2.1)\n",
    "- [6 - Testing with your Own Input](#6)\n",
    "- [7 - Word Embeddings](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc942dcd",
   "metadata": {
    "colab_type": "text",
    "id": "8aeCPdILgrga"
   },
   "source": [
    "In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n",
    "\n",
    "<center> <span style='color:blue'> <b>This movie was almost good.</b> </span> </center>\n",
    "\n",
    "Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: \n",
    "\n",
    "- Understand how you can build/design a model using layers\n",
    "- Train a model using a training loop\n",
    "- Use a binary cross-entropy loss function\n",
    "- Compute the accuracy of your model\n",
    "- Predict using your own input\n",
    "\n",
    "As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. \n",
    "- Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library `trax` that we use for building and training models.\n",
    "\n",
    "\n",
    "Now we will show you how to compute the gradient of a certain function `f` by just using `  .grad(f)`. \n",
    "\n",
    "- Trax source code can be found on Github: [Trax](https://github.com/google/trax)\n",
    "- The Trax code also uses the JAX library: [JAX](https://jax.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05815dbd",
   "metadata": {
    "colab_type": "text",
    "id": "IOK4n9JEjCVs"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Import Libraries and try out Trax\n",
    "\n",
    "- Let's import libraries and look at an example of using the Trax library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536f0f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trax\n",
      "  Using cached trax-1.4.1-py2.py3-none-any.whl (637 kB)\n",
      "Collecting absl-py (from trax)\n",
      "  Obtaining dependency information for absl-py from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: funcsigs in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trax) (1.0.2)\n",
      "Requirement already satisfied: gin-config in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trax) (0.5.0)\n",
      "Collecting gym (from trax)\n",
      "  Using cached gym-0.26.2-py3-none-any.whl\n",
      "Collecting jax (from trax)\n",
      "  Obtaining dependency information for jax from https://files.pythonhosted.org/packages/b5/5b/5131520dd9a384a640399e5efe4324fdee9e8a48685a33d08eb47140ccc3/jax-0.4.18-py3-none-any.whl.metadata\n",
      "  Using cached jax-0.4.18-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting jaxlib (from trax)\n",
      "  Obtaining dependency information for jaxlib from https://files.pythonhosted.org/packages/4d/af/22bf25b1b9c56a774d34eeac8f6d70c2e5d0a9d8b33b374e39517f830902/jaxlib-0.4.18-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached jaxlib-0.4.18-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting matplotlib (from trax)\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/40/d9/c1784db9db0d484c8e5deeafbaac0d6ed66e165c6eb4a74fb43a5fa947d9/matplotlib-3.8.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached matplotlib-3.8.0-cp311-cp311-win_amd64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trax) (1.26.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trax) (5.9.5)\n",
      "Collecting scipy (from trax)\n",
      "  Obtaining dependency information for scipy from https://files.pythonhosted.org/packages/81/d7/d2537d51efb692d0c411e64267ba349e7668d40f5bc73cefe78ccd650dcd/scipy-1.11.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached scipy-1.11.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: six in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from trax) (1.16.0)\n",
      "Collecting tensorflow-datasets (from trax)\n",
      "  Obtaining dependency information for tensorflow-datasets from https://files.pythonhosted.org/packages/a1/73/7a9ed7935f6833d73b32f1e2a1210082f5ccb95445440b4e2b0f66ab7792/tensorflow_datasets-4.9.3-py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_datasets-4.9.3-py3-none-any.whl.metadata (9.3 kB)\n",
      "INFO: pip is looking at multiple versions of trax to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting trax\n",
      "  Using cached trax-1.4.0-py2.py3-none-any.whl (637 kB)\n",
      "  Using cached trax-1.3.9-py2.py3-none-any.whl (629 kB)\n",
      "Collecting t5 (from trax)\n",
      "  Using cached t5-0.9.4-py2.py3-none-any.whl (164 kB)\n",
      "Collecting trax\n",
      "  Using cached trax-1.3.8-py2.py3-none-any.whl (617 kB)\n",
      "  Using cached trax-1.3.7-py2.py3-none-any.whl (521 kB)\n",
      "  Using cached trax-1.3.6-py2.py3-none-any.whl (468 kB)\n",
      "  Using cached trax-1.3.5-py2.py3-none-any.whl (416 kB)\n",
      "Collecting tensor2tensor (from trax)\n",
      "  Using cached tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
      "Collecting trax\n",
      "  Using cached trax-1.3.4-py2.py3-none-any.whl (366 kB)\n",
      "INFO: pip is still looking at multiple versions of trax to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached trax-1.3.3-py2.py3-none-any.whl (365 kB)\n",
      "  Using cached trax-1.3.2-py2.py3-none-any.whl (365 kB)\n",
      "  Using cached trax-1.3.1-py2.py3-none-any.whl (347 kB)\n",
      "  Using cached trax-1.3-py2.py3-none-any.whl (346 kB)\n",
      "  Using cached trax-1.2.4-py2.py3-none-any.whl (426 kB)\n",
      "Collecting cloudpickle>=1.2.0 (from gym->trax)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gym->trax) (0.0.8)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax->trax)\n",
      "  Obtaining dependency information for ml-dtypes>=0.2.0 from https://files.pythonhosted.org/packages/4f/95/48c66f80acb9f91c3c2fd0cc6939457b8b0c1bd0d2b96edb461a5209df80/ml_dtypes-0.3.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached ml_dtypes-0.3.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum (from jax->trax)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: bz2file in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor->trax) (0.98)\n",
      "Collecting dopamine-rl (from tensor2tensor->trax)\n",
      "  Using cached dopamine_rl-4.0.6-py3-none-any.whl (179 kB)\n",
      "Requirement already satisfied: flask in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor->trax) (2.3.2)\n",
      "Collecting future (from tensor2tensor->trax)\n",
      "  Using cached future-0.18.3-py3-none-any.whl\n",
      "Collecting gevent (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for gevent from https://files.pythonhosted.org/packages/a0/98/5a074e2b7006e627ea72e8be96d83801a2037bf60efd517e5d432aa93bd0/gevent-23.9.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached gevent-23.9.1-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting google-api-python-client (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for google-api-python-client from https://files.pythonhosted.org/packages/f8/e9/11fb73fe17f63e2e8aa7c1f4afb790ab737c0881dce4757bdead998ef48d/google_api_python_client-2.102.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_api_python_client-2.102.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting gunicorn (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for gunicorn from https://files.pythonhosted.org/packages/0e/2a/c3a878eccb100ccddf45c50b6b8db8cf3301a6adede6e31d48e8531cab13/gunicorn-21.2.0-py3-none-any.whl.metadata\n",
      "  Using cached gunicorn-21.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting h5py (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for h5py from https://files.pythonhosted.org/packages/d1/93/0f4cf5058095d749d464e4f770d2bf339930e5f3374331f0d2fa6ddfbf28/h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting kfac (from tensor2tensor->trax)\n",
      "  Using cached kfac-0.2.4-py2.py3-none-any.whl (193 kB)\n",
      "Collecting mesh-tensorflow (from tensor2tensor->trax)\n",
      "  Using cached mesh_tensorflow-0.1.21-py3-none-any.whl (385 kB)\n",
      "Collecting oauth2client (from tensor2tensor->trax)\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting opencv-python (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/38/d2/3e8c13ffc37ca5ebc6f382b242b44acb43eb489042e1728407ac3904e72f/opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata\n",
      "  Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Collecting Pillow (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for Pillow from https://files.pythonhosted.org/packages/54/9b/debe992677af84859ec1e38777b1d5c0671918188324153ecbc1f16f6cb6/Pillow-10.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached Pillow-10.0.1-cp311-cp311-win_amd64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: pypng in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor->trax) (0.20220715.0)\n",
      "Collecting requests (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: sympy in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor->trax) (1.12)\n",
      "Collecting tensorflow-addons (from tensor2tensor->trax)\n",
      "  Obtaining dependency information for tensorflow-addons from https://files.pythonhosted.org/packages/58/fd/f8b32ee1a9cd12ce1ab261abc66de860d0f63ff887dace00f0192f152e4f/tensorflow_addons-0.21.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_addons-0.21.0-cp311-cp311-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-gan (from tensor2tensor->trax)\n",
      "  Using cached tensorflow_gan-2.1.0-py2.py3-none-any.whl (367 kB)\n",
      "Collecting tensorflow-probability==0.7.0 (from tensor2tensor->trax)\n",
      "  Using cached tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
      "Collecting tf-slim (from tensor2tensor->trax)\n",
      "  Using cached tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensor2tensor->trax) (4.66.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-probability==0.7.0->tensor2tensor->trax) (5.1.1)\n",
      "Collecting array-record (from tensorflow-datasets->trax)\n",
      "  Obtaining dependency information for array-record from https://files.pythonhosted.org/packages/93/8e/c6b646029a9c544ecd6806c6ea0efb67d0ba3300e1d68518f58d9774d3f0/array_record-0.4.1-py310-none-any.whl.metadata\n",
      "  Using cached array_record-0.4.1-py310-none-any.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: click in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->trax) (8.1.6)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->trax) (0.1.8)\n",
      "Collecting etils[enp,epath,etree]>=0.9.0 (from tensorflow-datasets->trax)\n",
      "  Obtaining dependency information for etils[enp,epath,etree]>=0.9.0 from https://files.pythonhosted.org/packages/db/b1/094f147d77178d7a3553b40b6bc13fc9dd1ba4ef5425abe94d07ad2447dc/etils-1.5.0-py3-none-any.whl.metadata\n",
      "  Using cached etils-1.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting promise (from tensorflow-datasets->trax)\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets->trax)\n",
      "  Obtaining dependency information for protobuf>=3.20 from https://files.pythonhosted.org/packages/c2/59/f89c04923d68595d359f4cd7adbbdf5e5d791257945f8873d88b2fd1f979/protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Using cached protobuf-4.24.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Collecting tensorflow-metadata (from tensorflow-datasets->trax)\n",
      "  Obtaining dependency information for tensorflow-metadata from https://files.pythonhosted.org/packages/41/23/3705c7139886c079ef4c0e3be56a5a1fb90e9ee413a4b7caaee0ee0ea6fe/tensorflow_metadata-1.14.0-py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_metadata-1.14.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: termcolor in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->trax) (2.3.0)\n",
      "Requirement already satisfied: toml in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->trax) (0.10.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-datasets->trax) (1.14.1)\n",
      "Collecting fsspec (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->trax)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting importlib_resources (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->trax)\n",
      "  Obtaining dependency information for importlib_resources from https://files.pythonhosted.org/packages/65/6e/09d8816b5cb7a4006ef8ad1717a2703ad9f331dae9717d9f22488a2d6469/importlib_resources-6.1.0-py3-none-any.whl.metadata\n",
      "  Using cached importlib_resources-6.1.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->trax) (4.8.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->trax) (3.17.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->tensor2tensor->trax)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/92/5e/50028bbb269986d9bc30270cd46b47ea44a1ca0b3f8da3a8429680d37050/charset_normalizer-3.3.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached charset_normalizer-3.3.0-cp311-cp311-win_amd64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->tensor2tensor->trax)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->tensor2tensor->trax) (2.0.6)\n",
      "Collecting certifi>=2017.4.17 (from requests->tensor2tensor->trax)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->tensorflow-datasets->trax) (0.4.6)\n",
      "Collecting tensorflow>=2.2.0 (from dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for tensorflow>=2.2.0 from https://files.pythonhosted.org/packages/80/6f/57d36f6507e432d7fc1956b2e9e8530c5c2d2bfcd8821bcbfae271cd6688/tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow-2.14.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting gym (from trax)\n",
      "  Using cached gym-0.25.2.tar.gz (734 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting flax>=0.2.0 (from dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for flax>=0.2.0 from https://files.pythonhosted.org/packages/5c/69/6aaa77d3fa3599d64527196e0b231476fa2cffc4995675974e22d9df83e9/flax-0.7.4-py3-none-any.whl.metadata\n",
      "  Using cached flax-0.7.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pygame>=1.9.2 (from dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for pygame>=1.9.2 from https://files.pythonhosted.org/packages/82/61/93ae7afbd931a70510cfdf0a7bb0007540020b8d80bc1d8762ebdc46479b/pygame-2.5.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached pygame-2.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting pandas>=0.24.2 (from dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for pandas>=0.24.2 from https://files.pythonhosted.org/packages/2d/5e/9213ea10ac473e2437dc2cb17323ddc0999997e2713d6a0b683b10773994/pandas-2.1.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached pandas-2.1.1-cp311-cp311-win_amd64.whl.metadata (18 kB)\n",
      "INFO: pip is looking at multiple versions of dopamine-rl to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dopamine-rl (from tensor2tensor->trax)\n",
      "  Using cached dopamine_rl-4.0.5-py3-none-any.whl (174 kB)\n",
      "  Using cached dopamine_rl-4.0.2-py3-none-any.whl (164 kB)\n",
      "  Using cached dopamine_rl-4.0.1-py3-none-any.whl (151 kB)\n",
      "  Using cached dopamine_rl-4.0.0-py3-none-any.whl (147 kB)\n",
      "  Using cached dopamine_rl-3.2.1-py3-none-any.whl (127 kB)\n",
      "Requirement already satisfied: Werkzeug>=2.3.3 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor->trax) (2.3.6)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor->trax) (3.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor->trax) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flask->tensor2tensor->trax) (1.6.2)\n",
      "Requirement already satisfied: zope.event in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gevent->tensor2tensor->trax) (5.0)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gevent->tensor2tensor->trax) (6.1)\n",
      "Collecting greenlet>=3.0rc3 (from gevent->tensor2tensor->trax)\n",
      "  Obtaining dependency information for greenlet>=3.0rc3 from https://files.pythonhosted.org/packages/7f/ac/fd08b9c31d0b12c5797ea4303e156835bc645d8756106e165d86cfd444fd/greenlet-3.0.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached greenlet-3.0.0-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.12.2 (from gevent->tensor2tensor->trax)\n",
      "  Obtaining dependency information for cffi>=1.12.2 from https://files.pythonhosted.org/packages/5a/c7/694814b3757878b29da39bc2f0cf9d20295f4c1e0a0bde7971708d5f23f8/cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client->tensor2tensor->trax)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client->tensor2tensor->trax)\n",
      "  Obtaining dependency information for google-auth<3.0.0.dev0,>=1.19.0 from https://files.pythonhosted.org/packages/d7/88/1826b0c047c48763b36ed854a984127b430a16b70003155d7b19975f1d59/google_auth-2.23.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth-2.23.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client->tensor2tensor->trax)\n",
      "  Obtaining dependency information for google-auth-httplib2>=0.1.0 from https://files.pythonhosted.org/packages/d3/3d/e4991229886c0d522d9552151a43ff7adcc61e026e60ce8bd508387f84cf/google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata\n",
      "  Using cached google_auth_httplib2-0.1.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client->tensor2tensor->trax)\n",
      "  Obtaining dependency information for google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 from https://files.pythonhosted.org/packages/4d/ce/4fd62ea66b3508debc795e475336ce915929765870f0ad52328426ba016e/google_api_core-2.12.0-py3-none-any.whl.metadata\n",
      "  Using cached google_api_core-2.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->tensor2tensor->trax) (4.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gunicorn->tensor2tensor->trax) (23.2)\n",
      "INFO: pip is looking at multiple versions of kfac to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kfac (from tensor2tensor->trax)\n",
      "  Using cached kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n",
      "  Using cached kfac-0.2.2-py2.py3-none-any.whl (191 kB)\n",
      "  Using cached kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client->tensor2tensor->trax)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client->tensor2tensor->trax)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa>=3.1.4 (from oauth2client->tensor2tensor->trax)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->tensor2tensor->trax) (1.3.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-addons->tensor2tensor->trax) (2.13.3)\n",
      "Collecting tensorflow-hub>=0.2 (from tensorflow-gan->tensor2tensor->trax)\n",
      "  Obtaining dependency information for tensorflow-hub>=0.2 from https://files.pythonhosted.org/packages/6e/1a/fbae76f4057b9bcdf9468025d7a8ca952dec14bfafb9fc0b1e4244ce212f/tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting absl-py (from trax)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow-datasets->trax)\n",
      "  Obtaining dependency information for googleapis-common-protos<2,>=1.52.0 from https://files.pythonhosted.org/packages/a7/bc/416a1ffeba4dcd072bc10523dac9ed97f2e7fc4b760580e2bdbdc1e2afdd/googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf>=3.20 (from tensorflow-datasets->trax)\n",
      "  Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Collecting pycparser (from cffi>=1.12.2->gevent->tensor2tensor->trax)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Collecting msgpack (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for msgpack from https://files.pythonhosted.org/packages/b4/3d/c8dd23050eefa3d9b9c5b8329ed3308c2f2f80f65825e9ea4b7fa621cdab/msgpack-1.0.7-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached msgpack-1.0.7-cp311-cp311-win_amd64.whl.metadata (9.4 kB)\n",
      "Collecting optax (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for optax from https://files.pythonhosted.org/packages/13/71/787cc24c4b606f3bb9f1d14957ebd7cb9e4234f6d59081721230b2032196/optax-0.1.7-py3-none-any.whl.metadata\n",
      "  Using cached optax-0.1.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting orbax-checkpoint (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for orbax-checkpoint from https://files.pythonhosted.org/packages/7b/77/81260a56b046cb95d97a2700687ae5a7b9368fa8a7726f1864c1c368fa0b/orbax_checkpoint-0.4.1-py3-none-any.whl.metadata\n",
      "  Using cached orbax_checkpoint-0.4.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tensorstore in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax) (0.1.45)\n",
      "Collecting rich>=11.1 (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for rich>=11.1 from https://files.pythonhosted.org/packages/be/2a/4e62ff633612f746f88618852a626bbe24226eba5e7ac90e91dcfd6a414e/rich-13.6.0-py3-none-any.whl.metadata\n",
      "  Using cached rich-13.6.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting PyYAML>=5.4.1 (from flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for PyYAML>=5.4.1 from https://files.pythonhosted.org/packages/b3/34/65bb4b2d7908044963ebf614fe0fdb080773fc7030d7e39c8d3eddcd4257/PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client->tensor2tensor->trax)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting ale-py~=0.8.0 (from gym->trax)\n",
      "  Using cached ale_py-0.8.1-cp311-cp311-win_amd64.whl (952 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.15.0->google-api-python-client->tensor2tensor->trax)\n",
      "  Obtaining dependency information for pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 from https://files.pythonhosted.org/packages/39/92/8486ede85fcc088f1b3dba4ce92dd29d126fd96b0008ea213167940a2475/pyparsing-3.1.1-py3-none-any.whl.metadata\n",
      "  Using cached pyparsing-3.1.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from Jinja2>=3.1.2->flask->tensor2tensor->trax) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor->trax) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor->trax) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24.2->dopamine-rl->tensor2tensor->trax) (2023.3)\n",
      "Collecting tensorflow-intel==2.14.0 (from tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for tensorflow-intel==2.14.0 from https://files.pythonhosted.org/packages/ad/6e/1bfe367855dd87467564f7bf9fa14f3b17889988e79598bc37bf18f5ffb6/tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax) (23.5.26)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Using cached gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax) (16.0.6)\n",
      "Collecting ml-dtypes>=0.2.0 (from jax->trax)\n",
      "  Obtaining dependency information for ml-dtypes>=0.2.0 from https://files.pythonhosted.org/packages/08/89/c727fde1a3d12586e0b8c01abf53754707d76beaa9987640e70807d4545f/ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.1776.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax) (65.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax) (0.31.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/75/c5/fb3ed7495c73c0de58b08376a468a35bdb61b89ddfbdb96a37bceb54f959/grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached grpcio-1.59.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.15,>=2.14 (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for tensorboard<2.15,>=2.14 from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax) (2.14.0)\n",
      "Collecting keras<2.15,>=2.14.0 (from tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for keras<2.15,>=2.14.0 from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor->trax) (2.16.1)\n",
      "Collecting chex>=0.1.5 (from optax->flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for chex>=0.1.5 from https://files.pythonhosted.org/packages/8b/c5/ab99c61d1384f89fe0d89b4b105c1ad22dab98cfe8c78136fb8c3f75f75b/chex-0.1.83-py3-none-any.whl.metadata\n",
      "  Using cached chex-0.1.83-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from orbax-checkpoint->flax>=0.2.0->dopamine-rl->tensor2tensor->trax) (1.5.8)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax) (0.41.2)\n",
      "Requirement already satisfied: toolz>=0.9.0 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chex>=0.1.5->optax->flax>=0.2.0->dopamine-rl->tensor2tensor->trax) (0.12.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\n",
      "  Using cached Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\skand\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax) (0.7.1)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-intel==2.14.0->tensorflow>=2.2.0->dopamine-rl->tensor2tensor->trax)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached jax-0.4.18-py3-none-any.whl (1.7 MB)\n",
      "Using cached scipy-1.11.3-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "Using cached jaxlib-0.4.18-cp311-cp311-win_amd64.whl (46.6 MB)\n",
      "Using cached tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Using cached array_record-0.4.1-py310-none-any.whl (3.0 MB)\n",
      "Using cached opencv_python-4.8.1.78-cp37-abi3-win_amd64.whl (38.1 MB)\n",
      "Using cached Pillow-10.0.1-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "Using cached gevent-23.9.1-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached google_api_python_client-2.102.0-py2.py3-none-any.whl (12.4 MB)\n",
      "Using cached gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "Using cached h5py-3.9.0-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached tensorflow_addons-0.21.0-cp311-cp311-win_amd64.whl (719 kB)\n",
      "Using cached tensorflow_metadata-1.14.0-py3-none-any.whl (28 kB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached cffi-1.16.0-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Using cached charset_normalizer-3.3.0-cp311-cp311-win_amd64.whl (97 kB)\n",
      "Using cached flax-0.7.4-py3-none-any.whl (233 kB)\n",
      "Using cached google_api_core-2.12.0-py3-none-any.whl (121 kB)\n",
      "Using cached google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "Using cached google_auth_httplib2-0.1.1-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "Using cached greenlet-3.0.0-cp311-cp311-win_amd64.whl (288 kB)\n",
      "Using cached pandas-2.1.1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "Using cached pygame-2.5.2-cp311-cp311-win_amd64.whl (10.8 MB)\n",
      "Using cached tensorflow-2.14.0-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Using cached tensorflow_intel-2.14.0-cp311-cp311-win_amd64.whl (284.2 MB)\n",
      "Using cached ml_dtypes-0.2.0-cp311-cp311-win_amd64.whl (938 kB)\n",
      "Using cached tensorflow_hub-0.15.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached etils-1.5.0-py3-none-any.whl (140 kB)\n",
      "Using cached fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "Using cached importlib_resources-6.1.0-py3-none-any.whl (33 kB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Using cached pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Using cached rich-13.6.0-py3-none-any.whl (239 kB)\n",
      "Using cached msgpack-1.0.7-cp311-cp311-win_amd64.whl (222 kB)\n",
      "Using cached optax-0.1.7-py3-none-any.whl (154 kB)\n",
      "Using cached orbax_checkpoint-0.4.1-py3-none-any.whl (113 kB)\n",
      "Using cached chex-0.1.83-py3-none-any.whl (94 kB)\n",
      "Using cached grpcio-1.59.0-cp311-cp311-win_amd64.whl (3.7 MB)\n",
      "Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "Using cached Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "Installing collected packages: tensorflow-addons, scipy, PyYAML, pyparsing, pygame, pycparser, pyasn1, protobuf, promise, Pillow, opt-einsum, opencv-python, oauthlib, msgpack, ml-dtypes, mdurl, markdown, keras, importlib_resources, idna, h5py, gunicorn, grpcio, greenlet, google-pasta, gast, future, fsspec, etils, cloudpickle, charset-normalizer, certifi, cachetools, astunparse, absl-py, tf-slim, tensorflow-probability, tensorflow-hub, rsa, requests, pyasn1-modules, pandas, mesh-tensorflow, markdown-it-py, jaxlib, jax, httplib2, gym, googleapis-common-protos, cffi, ale-py, tensorflow-metadata, tensorflow-gan, rich, requests-oauthlib, oauth2client, kfac, google-auth, gevent, chex, orbax-checkpoint, optax, google-auth-oauthlib, google-auth-httplib2, google-api-core, array-record, tensorboard, google-api-python-client, flax, tensorflow-intel, tensorflow-datasets, tensorflow, dopamine-rl, tensor2tensor, trax\n",
      "Successfully installed Pillow-10.0.1 PyYAML-6.0.1 absl-py-1.4.0 ale-py-0.8.1 array-record-0.4.1 astunparse-1.6.3 cachetools-5.3.1 certifi-2023.7.22 cffi-1.16.0 charset-normalizer-3.3.0 chex-0.1.83 cloudpickle-2.2.1 dopamine-rl-3.2.1 etils-1.5.0 flax-0.7.4 fsspec-2023.9.2 future-0.18.3 gast-0.5.4 gevent-23.9.1 google-api-core-2.12.0 google-api-python-client-2.102.0 google-auth-2.23.2 google-auth-httplib2-0.1.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 googleapis-common-protos-1.60.0 greenlet-3.0.0 grpcio-1.59.0 gunicorn-21.2.0 gym-0.26.2 h5py-3.9.0 httplib2-0.22.0 idna-3.4 importlib_resources-6.1.0 jax-0.4.18 jaxlib-0.4.18 keras-2.14.0 kfac-0.2.0 markdown-3.5 markdown-it-py-3.0.0 mdurl-0.1.2 mesh-tensorflow-0.1.21 ml-dtypes-0.2.0 msgpack-1.0.7 oauth2client-4.1.3 oauthlib-3.2.2 opencv-python-4.8.1.78 opt-einsum-3.3.0 optax-0.1.7 orbax-checkpoint-0.4.1 pandas-2.1.1 promise-2.3 protobuf-3.20.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycparser-2.21 pygame-2.5.2 pyparsing-3.1.1 requests-2.31.0 requests-oauthlib-1.3.1 rich-13.6.0 rsa-4.9 scipy-1.11.3 tensor2tensor-1.15.7 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-addons-0.21.0 tensorflow-datasets-4.9.3 tensorflow-gan-2.1.0 tensorflow-hub-0.15.0 tensorflow-intel-2.14.0 tensorflow-metadata-1.14.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 trax-1.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74a4623",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "WOTfm2P0jCVt",
    "outputId": "d4903011-7268-4bea-ae35-ea3fd0b46311"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'trax'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skand\\Documents\\GitHub\\deeplearning.ai\\4 Natural Language Processing\\3 NLP with Sequence Models\\1 Neural Networks for sentiment analysis\\3 sentiment analysis\\C3_W1_Assignment.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skand/Documents/GitHub/deeplearning.ai/4%20Natural%20Language%20Processing/3%20NLP%20with%20Sequence%20Models/1%20Neural%20Networks%20for%20sentiment%20analysis/3%20sentiment%20analysis/C3_W1_Assignment.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mrnd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skand/Documents/GitHub/deeplearning.ai/4%20Natural%20Language%20Processing/3%20NLP%20with%20Sequence%20Models/1%20Neural%20Networks%20for%20sentiment%20analysis/3%20sentiment%20analysis/C3_W1_Assignment.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# import relevant libraries\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/skand/Documents/GitHub/deeplearning.ai/4%20Natural%20Language%20Processing/3%20NLP%20with%20Sequence%20Models/1%20Neural%20Networks%20for%20sentiment%20analysis/3%20sentiment%20analysis/C3_W1_Assignment.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtrax\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skand/Documents/GitHub/deeplearning.ai/4%20Natural%20Language%20Processing/3%20NLP%20with%20Sequence%20Models/1%20Neural%20Networks%20for%20sentiment%20analysis/3%20sentiment%20analysis/C3_W1_Assignment.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtrax\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfastmath\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skand/Documents/GitHub/deeplearning.ai/4%20Natural%20Language%20Processing/3%20NLP%20with%20Sequence%20Models/1%20Neural%20Networks%20for%20sentiment%20analysis/3%20sentiment%20analysis/C3_W1_Assignment.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrax\u001b[39;00m \u001b[39mimport\u001b[39;00m layers \u001b[39mas\u001b[39;00m tl\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'trax'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import shutil\n",
    "import random as rnd\n",
    "\n",
    "# import relevant libraries\n",
    "import trax\n",
    "import trax.fastmath.numpy as np\n",
    "from trax import layers as tl\n",
    "from trax import fastmath\n",
    "\n",
    "# import Layer from the utils.py file\n",
    "from utils import Layer, load_tweets, process_tweet\n",
    "import w1_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a4683",
   "metadata": {
    "colab_type": "text",
    "id": "CZ8RUynQsktn"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Importing the Data\n",
    "\n",
    "<a name=\"2-1\"></a>\n",
    "### 2.1 - Loading in the Data\n",
    "\n",
    "- implementation of `process_tweets` function is available in utils.py file, it removes unwanted characters e.g. hashtag, hyperlinks, stock tickers from a tweet.\n",
    "- It also returns a list of words (it tokenizes the original string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00a7b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "h5ClwIOSuLJh",
    "outputId": "dee50364-c476-405f-8cdd-63d067b848d6"
   },
   "outputs": [],
   "source": [
    "def train_val_split():\n",
    "    # Load positive and negative tweets\n",
    "    all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "    # View the total number of positive and negative tweets.\n",
    "    print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "    print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "    # Split positive set into validation and training\n",
    "    val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "    train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "    # Split negative set into validation and training\n",
    "    val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "    train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "    \n",
    "    # Combine training data into one set\n",
    "    train_x = train_pos + train_neg \n",
    "\n",
    "    # Combine validation data into one set\n",
    "    val_x  = val_pos + val_neg\n",
    "\n",
    "    # Set the labels for the training set (1 for positive, 0 for negative)\n",
    "    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "    # Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "    val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "\n",
    "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_val_split()\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66124fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "2bRX6aPDjCWH",
    "outputId": "09497362-bf73-4d63-e99f-460027e91eb5"
   },
   "outputs": [],
   "source": [
    "# Try out function that processes tweets\n",
    "print(\"original tweet at training position 0\")\n",
    "print(train_pos[0])\n",
    "\n",
    "print(\"Tweet at training position 0 after processing:\")\n",
    "process_tweet(train_pos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa23f87",
   "metadata": {
    "colab_type": "text",
    "id": "ac4D5WSUAVub"
   },
   "source": [
    "<a name=\"2-2\"></a>\n",
    "### 2.2 - Building the Vocabulary\n",
    "\n",
    "Now build the vocabulary.\n",
    "- Map each word in each tweet to an integer (an \"index\"), we will build it based on the training data. We will do that by assigning an index to everyword by iterating over the training set.\n",
    "\n",
    "The vocabulary will also include some special tokens\n",
    "- `__PAD__`: padding\n",
    "- `</e>`: end of line\n",
    "- `__UNK__`: a token representing any word that is not in the vocabulary.\n",
    "\n",
    "The dictionary `Vocab` will look like this:\n",
    "```\n",
    "{'__PAD__': 0,\n",
    " '__</e>__': 1,\n",
    " '__UNK__': 2,\n",
    " 'followfriday': 3,\n",
    " 'top': 4,\n",
    " 'engag': 5,\n",
    "```\n",
    "\n",
    "- So Each unique word has a unique integer associated with it.\n",
    "- The total number of words in Vocab: 9088"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e61da",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rQaHKs7kAVuc",
    "outputId": "1d360f94-902d-471c-d3c3-0d69c27f5eaf"
   },
   "outputs": [],
   "source": [
    "# Build the vocabulary\n",
    "# There is no test set here only train/val\n",
    "def get_vocab(train_x):\n",
    "\n",
    "    # Include special tokens \n",
    "    # started with pad, end of line and unk tokens\n",
    "    Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "    for tweet in train_x: \n",
    "        processed_tweet = process_tweet(tweet)\n",
    "        for word in processed_tweet:\n",
    "            if word not in Vocab: \n",
    "                Vocab[word] = len(Vocab)\n",
    "    \n",
    "    return Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = get_vocab(train_x)\n",
    "\n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "display(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aef416",
   "metadata": {
    "colab_type": "text",
    "id": "0x8pND8tAVuf"
   },
   "source": [
    "<a name=\"2-3\"></a>\n",
    "## 2.3 - Converting a Tweet to a Tensor\n",
    "\n",
    "the next function does the following, given an example\n",
    "\n",
    "##### Example\n",
    "Input a tweet:\n",
    "```CPP\n",
    "'@happypuppy, is Maria happy?'\n",
    "\n",
    "```\n",
    "\n",
    "the function first converts the tweet into a list of tokens (including only relevant words)\n",
    "```CPP\n",
    "['maria', 'happi']\n",
    "```\n",
    "\n",
    "Then it will convert each word into its unique integer from the vocabulary \n",
    "\n",
    "```CPP\n",
    "[2, 56]\n",
    "```\n",
    "- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the `__UNK__` token, because it is considered \"unknown.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f51d42",
   "metadata": {
    "colab_type": "text",
    "id": "QtQhtv0kjCWQ"
   },
   "source": [
    "<a name=\"2.3.1\"></a>\n",
    "### 2.3.1 - tweet_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e25a9",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ft1zNGMaAVuf"
   },
   "outputs": [],
   "source": [
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''     \n",
    "    # Process the tweet into a list of words\n",
    "    # where only important words are kept (stop words removed)\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = [] \n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.     \n",
    "\n",
    "        word_ID = vocab_dict.get(word, vocab_dict[unk_token])\n",
    "\n",
    "        #equivalent to \n",
    "        #if word not in vocab_dict:\n",
    "        #    word = unk_token\n",
    "            \n",
    "        #word_ID = vocab_dict[word]\n",
    "\n",
    "            \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID)\n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bffc268",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ze0Zx_5UjCWU",
    "outputId": "a9427907-b364-4c5f-fed5-bd2502e18bab"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\skand\\Documents\\GitHub\\deeplearning.ai\\4 Natural Language Processing\\3 NLP with Sequence Models\\1 Neural Networks for sentiment analysis\\3 sentiment analysis\\C3_W1_Assignment.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/skand/Documents/GitHub/deeplearning.ai/4%20Natural%20Language%20Processing/3%20NLP%20with%20Sequence%20Models/1%20Neural%20Networks%20for%20sentiment%20analysis/3%20sentiment%20analysis/C3_W1_Assignment.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mActual tweet is\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, val_pos[\u001b[39m0\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/skand/Documents/GitHub/deeplearning.ai/4%20Natural%20Language%20Processing/3%20NLP%20with%20Sequence%20Models/1%20Neural%20Networks%20for%20sentiment%20analysis/3%20sentiment%20analysis/C3_W1_Assignment.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTensor of tweet:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, tweet_to_tensor(val_pos[\u001b[39m0\u001b[39m], vocab_dict\u001b[39m=\u001b[39mVocab))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'val_pos' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4eb0fc",
   "metadata": {
    "colab_type": "text",
    "id": "rwAZZIYYAVuj"
   },
   "source": [
    "<a name=\"2-4\"></a>\n",
    "### 2.4 - Creating a Batch Generator\n",
    "\n",
    "Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. \n",
    "- If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. \n",
    "- So we will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to treat some examples as more important to get right than others, but commonly this will all be 1.0). \n",
    "\n",
    "Once we create the generator, we can use it in a for loop\n",
    "\n",
    "```CPP\n",
    "for batch_inputs, batch_targets, batch_example_weights in data_generator:\n",
    "    forward_propagate\n",
    "    ...\n",
    "```\n",
    "\n",
    "We can also get a single batch like this:\n",
    "\n",
    "```CPP\n",
    "batch_inputs, batch_targets, batch_example_weights = next(data_generator)\n",
    "```\n",
    "\n",
    "The generator returns the next batch each time it's called. \n",
    "- This generator returns the data in a format (tensors) that could directly be used in our model.\n",
    "- It returns a triplet: the inputs, targets, and loss weights:\n",
    "    - Inputs is a tensor that contains the batch of tweets we put into the model.\n",
    "    - Targets is the corresponding batch of labels that we train to generate.\n",
    "    - Loss weights here are just 1s with same shape as targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c81619",
   "metadata": {
    "colab_type": "text",
    "id": "sR-sF_o0jCWa"
   },
   "source": [
    "<a name=\"2.4.1\"></a>\n",
    "### 2.4.1 - data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb81aba",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPd9HNT7AVuk"
   },
   "outputs": [],
   "source": [
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "        data_pos - Set of positive examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch. Must be even\n",
    "        loop - True or False\n",
    "        vocab_dict - The words dictionary\n",
    "        shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "        inputs - Subset of positive and negative examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - A numpy array specifying the importance of each example\n",
    "        \n",
    "    '''     \n",
    "\n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples    \n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk through the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        \n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        \n",
    "        # Start from 0 and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            # If the positive index goes past the positive dataset,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "\n",
    "        # Using the same batch list, start from 0 and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            # If the negative index goes past the negative dataset,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True \n",
    "                    break \n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "                    \n",
    "            # get the tweet as neg_index\n",
    "            tweet =  data_neg[pos_index_lines[neg_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment neg_index by one\n",
    "            neg_index += 1\n",
    "\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (we will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = [0] * n_pad\n",
    "            \n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = [1] * n_to_take\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of zeros)\n",
    "        # The length is the number of negative examples in the batch\n",
    "        target_neg = [0] * n_to_take\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de71444",
   "metadata": {
    "colab_type": "text",
    "id": "kI9gEdqpjCWd"
   },
   "source": [
    "Now we can use our function to create a data generator for the training data, and another data generator for the validation data.\n",
    "\n",
    "We will create a third data generator that does not loop, for testing the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e11d5d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "iIwM4YHtAVum",
    "outputId": "d25aa4ae-bad3-41f5-963c-0a091f8fb108"
   },
   "outputs": [],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "\n",
    "def train_generator(batch_size,\n",
    "                    train_pos,\n",
    "                    train_neg,\n",
    "                    vocab_dict,\n",
    "                    loop=True, \n",
    "                    shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size,\n",
    "                    val_pos,\n",
    "                    val_neg, \n",
    "                    vocab_dict,\n",
    "                    loop=True,\n",
    "                    shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, \n",
    "                    val_pos,\n",
    "                    val_neg,   \n",
    "                    vocab_dict, \n",
    "                    loop=False,\n",
    "                    shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, loop, vocab_dict, shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eb77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, train_pos, train_neg, Vocab, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f02dd",
   "metadata": {
    "colab_type": "text",
    "id": "J3HrgcJmAVup"
   },
   "source": [
    "Now that we have your train/val generators, you can just call them and they will return tensors which correspond to the tweets in the first column and their corresponding labels in the second column(positive or negative). \n",
    ">Now we will go ahead and start building our neural network model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c819c53",
   "metadata": {
    "colab_type": "text",
    "id": "X591GrH_stXq"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Defining Classes\n",
    "\n",
    "In this part, we will write our own small framework (library of layers). It will be very similar\n",
    "to the one used in Trax and also in Keras and PyTorch. \n",
    "\n",
    "Your framework will be based on the following `Layer` class from utils.py.\n",
    "\n",
    "```CPP\n",
    "class Layer(object):\n",
    "    \"\"\" Base class for layers.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # set weights to None\n",
    "        self.weights = None\n",
    "\n",
    "    # The forward propagation should be implemented\n",
    "    # by subclasses of this Layer class\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # This function initializes the weights\n",
    "    # based on the input signature and random key,\n",
    "    # should be implemented by subclasses of this Layer class\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        pass\n",
    "\n",
    "    # This initializes and returns the weights, do not override.\n",
    "    def init(self, input_signature, random_key):\n",
    "        self.init_weights_and_state(input_signature, random_key)\n",
    "        return self.weights\n",
    " \n",
    "    # __call__ allows an object of this class\n",
    "    # to be called like it's a function.\n",
    "    def __call__(self, x):\n",
    "        # When this layer object is called, \n",
    "        # it calls its forward propagation function\n",
    "        return self.forward(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcWUXFaPzS-m"
   },
   "source": [
    "<a name=\"3-1\"></a>\n",
    "### 3.1 - ReLU Class\n",
    "We will now implement the ReLU activation function in a class below. The ReLU function looks as follows: \n",
    "<img src = \"images/relu.jpg\" style=\"width:300px;height:150px;\"/>\n",
    "\n",
    "$$ \\mathrm{ReLU}(x) = \\mathrm{max}(0,x) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGE5zZ5mzF9x"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Relu(Layer):\n",
    "    \"\"\"Relu activation function implementation\"\"\"\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input: \n",
    "            - x (a numpy array): the input\n",
    "        Output:\n",
    "            - activation (numpy array): all positive or 0 version of x\n",
    "        '''\n",
    "        \n",
    "        activation = np.maximum(x,0)\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XepjDxCQ1G8p"
   },
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 - Dense Class \n",
    "\n",
    "- The forward function of the Dense class multiplies the input to the layer (`x`) by the weight matrix (`W`)\n",
    "\n",
    "$$\\mathrm{forward}(\\mathbf{x},\\mathbf{W}) = \\mathbf{xW} $$\n",
    "\n",
    "we use  the trax version of `math`, for more efficient code execution,  which includes a trax version of `numpy` and also `random`.\n",
    "\n",
    "- Weights are initialized with a random key.\n",
    "- The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)\n",
    "- The num of rows for weights is equal to the number of columns in x, because for forward propagation, you will multiply x times weights.\n",
    "\n",
    "we use `trax.fastmath.random.normal(key, shape, dtype=tf.float32)` to generate random values for the weight matrix. The key difference between this function and the standard `numpy` randomness is the explicit use of random keys, which need to be passed. \n",
    "- `key` can be generated by calling `random.get_prng(seed=)` and passing in a number for the `seed`.\n",
    "- `shape` is a tuple with the desired shape of the weight matrix.\n",
    "    - The number of rows in the weight matrix should equal the number of columns in the variable `x`.  Since `x` may have 2 dimensions if it represents a single training example (row, col), or three dimensions (batch_size, row, col), we get the last dimension using shape[-1] and not to always get the last dimension from the tuple that holds the dimensions of x.\n",
    "    - The number of columns in the weight matrix is the number of units chosen for that dense layer.\n",
    "    \n",
    "- The values generated have a mean of 0 and standard deviation of 1 (standard normal distribution) then multiplied by a standard deviation of the random values to 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpiJ87L9jCWw"
   },
   "source": [
    "<a name=\"3.3.1\"></a>\n",
    "### 3.3.1 - Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "783FfWt70660"
   },
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    A dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # __init__ is implemented for you\n",
    "    def __init__(self, n_units, init_stdev=0.1):\n",
    "        \n",
    "        # Set the number of units in this layer\n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = init_stdev\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Matrix multiply x and the weight matrix\n",
    "        dense = np.dot(x,self.weights)\n",
    "        \n",
    "        return dense\n",
    "\n",
    "    # init_weights\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        \n",
    "        # The input_signature has a .shape attribute that gives the shape as a tuple\n",
    "        input_shape = input_signature.shape\n",
    "        \n",
    "        # Generate the weight matrix from a normal distribution, \n",
    "        # and standard deviation of 'stdev'        \n",
    "        w = self._init_stdev * trax.fastmath.random.normal(key=random_key, shape=(input_shape[-1], self._n_units))\n",
    "\n",
    "        self.weights = w\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZEY8vBCgrgy"
   },
   "source": [
    "<a name=\"3-3\"></a>\n",
    "### 3.3 - Model\n",
    "\n",
    "Now we will implement a classifier using neural networks. Here is the model architecture we will build. \n",
    "\n",
    "<img src = \"images/nn.jpg\" style=\"width:400px;height:250px;\"/>\n",
    "\n",
    "For the model implementation, we will use the Trax `layers` module, imported as `tl`.\n",
    "Trax layers are very similar to the ones we implemented(Relu and Dense) above,\n",
    "but in addition trainable weights can also have a non-trainable state.\n",
    "State is used in layers like batch normalization and for inference.\n",
    "\n",
    "First, look at the code of the Trax Dense layer and compare to your implementation above.\n",
    "- [tl.Dense](https://github.com/google/trax/blob/master/trax/layers/core.py#L29): Trax Dense layer implementation\n",
    "\n",
    "One other important layer that you will use a lot is one that allows to execute one layer after another in sequence.\n",
    "- [tl.Serial](https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26): Combinator that applies layers serially.  \n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))`\n",
    "\n",
    "- [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Layer constructor function for an embedding layer.  \n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).  \n",
    "\n",
    "- [tl.Mean](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276): Calculates means across an axis.  In this case, we chose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the sentence).  \n",
    "- For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements.  \n",
    "\n",
    "**Online documentation**\n",
    "\n",
    "- [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)\n",
    "\n",
    "- [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)\n",
    "\n",
    "- [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "\n",
    "- [tl.Mean](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8ONXnJsjCXH"
   },
   "source": [
    "<a name=\"3.3.1\"></a>\n",
    "### 3.3.1 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wh33Hk8lgrgz"
   },
   "outputs": [],
   "source": [
    "def classifier(vocab_size=9088, embedding_dim=256, output_dim=2, mode='train'):\n",
    "            \n",
    "    # create embedding layer\n",
    "    embed_layer = tl.Embedding( \n",
    "        vocab_size=vocab_size, # Size of the vocabulary\n",
    "        d_feature=embedding_dim # Embedding dimension\n",
    "    ) \n",
    "    \n",
    "    # Create a mean layer, to create an \"average\" word embedding\n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    \n",
    "    # Create a dense layer, one unit for each output\n",
    "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
    "    \n",
    "    # Use tl.Serial to combine all layers and create the classifier\n",
    "    model = tl.Serial( \n",
    "      embed_layer,\n",
    "      mean_layer,\n",
    "      dense_output_layer \n",
    "      ) \n",
    "    \n",
    "    # return the model of type\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FaugA_7grg6"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Training\n",
    "\n",
    "To train a model on a task, Trax defines an abstraction [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) which packages the train data, loss and optimizer (among other things) together into an object.\n",
    "\n",
    "Similarly to evaluate a model, Trax defines an abstraction [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) which packages the eval data and metrics (among other things) into another object.\n",
    "\n",
    "The final piece tying things together is the [`trax.supervised.training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.\n",
    "\n",
    "Using `Loop` will save us a lot of code compared to always writing the training loop by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "UGgKw03jjCXP",
    "outputId": "014a4326-53ac-4408-878f-0202f6c4828e"
   },
   "outputs": [],
   "source": [
    "# View documentation for trax.supervised.training.TrainTask\n",
    "help(trax.supervised.training.TrainTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "Tr2MmdWDn6hV",
    "outputId": "daec4adb-694d-407e-f1cc-8eb26628ed05"
   },
   "outputs": [],
   "source": [
    "# View documentation for trax.supervised.training.EvalTask\n",
    "help(trax.supervised.training.EvalTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XkUVMzVXn_8f",
    "outputId": "b5bdbd12-ec1c-4a4c-99ac-a122e5534434"
   },
   "outputs": [],
   "source": [
    "# View documentation for trax.supervised.training.Loop\n",
    "help(trax.supervised.training.Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "Ooekq1F305bt",
    "outputId": "9f60f810-9d6a-47b6-e977-b6a3c478e9ba"
   },
   "outputs": [],
   "source": [
    "# View optimizers that you could choose from\n",
    "help(trax.optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmR3BhV41Cxs"
   },
   "source": [
    "Notice some available optimizers include:\n",
    "```CPP\n",
    "    adafactor\n",
    "    adam\n",
    "    momentum\n",
    "    rms_prop\n",
    "    sm3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HA01H6K7grg_"
   },
   "source": [
    "<a name=\"4-1\"></a>\n",
    "### 4.1  Training the Model\n",
    "\n",
    "Now you are going to train the model. \n",
    "\n",
    "Let's define the `TrainTask`, `EvalTask` and `Loop` in preparation to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "ogMtJgHSoiZj"
   },
   "outputs": [],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "def get_train_eval_tasks(train_pos,\n",
    "                         train_neg,\n",
    "                         val_pos, \n",
    "                         val_neg, \n",
    "                         vocab_dict, \n",
    "                         loop, \n",
    "                         batch_size = 16):\n",
    "    \n",
    "    rnd.seed(271)\n",
    "\n",
    "    train_task = training.TrainTask(\n",
    "        labeled_data=train_generator(\n",
    "                                    batch_size,\n",
    "                                    train_pos,\n",
    "                                    train_neg, \n",
    "                                    vocab_dict,\n",
    "                                    loop, \n",
    "                                    shuffle = True),\n",
    "        loss_layer=tl.WeightedCategoryCrossEntropy(),\n",
    "        optimizer=trax.optimizers.Adam(0.01),\n",
    "        n_steps_per_checkpoint=10,\n",
    "    )\n",
    "\n",
    "    eval_task = training.EvalTask(\n",
    "        labeled_data=val_generator(\n",
    "                                    batch_size, \n",
    "                                    val_pos,\n",
    "                                    val_neg, \n",
    "                                    vocab_dict, \n",
    "                                    loop, \n",
    "                                    shuffle = True),\n",
    "                                    \n",
    "        metrics=[tl.WeightedCategoryCrossEntropy(),tl.WeightedCategoryAccuracy()],\n",
    "    )\n",
    "    \n",
    "    return train_task, eval_task\n",
    "    \n",
    "\n",
    "train_task, eval_task = get_train_eval_tasks(train_pos, train_neg, val_pos, val_neg, Vocab, True, batch_size = 16)\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_sw8EGd0Sjk"
   },
   "source": [
    "This defines a model trained using [`tl.WeightedCategoryCrossEntropy`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.WeightedCategoryCrossEntropy) optimized with the [`trax.optimizers.Adam`](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam) optimizer, all the while tracking the accuracy using [`tl.WeightedCategoryAccuracy`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.WeightedCategoryAccuracy) metric. We also track `tl.WeightedCategoryCrossEntropy` on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yB78IIUerIVG"
   },
   "source": [
    "Now let's make an output directory and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CNx4LnP9rMsO",
    "outputId": "359fab84-7b89-4eea-b64e-5c681f6952c1"
   },
   "outputs": [],
   "source": [
    "dir_path = './model/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    pass\n",
    "\n",
    "\n",
    "output_dir = './model/'\n",
    "output_dir_expand = os.path.expanduser(output_dir)\n",
    "print(output_dir_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4R4EHUcrwqe"
   },
   "source": [
    "<a name=\"4.1.1\"></a>\n",
    "### 4.1.1 - train_model\n",
    "\n",
    "> Now we will implement `train_model` to train the model ( the `classifier` that we wrote previously) for the given number of training steps (`n_steps`) using `TrainTask`, `EvalTask` and `Loop`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tolygrj7rpFX"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    '''\n",
    "    Input: \n",
    "        classifier - the model you are building\n",
    "        train_task - Training task\n",
    "        eval_task - Evaluation task. Received as a list.\n",
    "        n_steps - the evaluation steps\n",
    "        output_dir - folder to save your files\n",
    "    Output:\n",
    "        trainer -  trax trainer\n",
    "    '''\n",
    "    rnd.seed(31) #\n",
    "    \n",
    "    training_loop = training.Loop( \n",
    "                                classifier, # The learning model\n",
    "                                train_task, # The training task\n",
    "                                eval_tasks=eval_task, # The evaluation task\n",
    "                                output_dir=output_dir, # The output directory\n",
    "                                random_seed=31 \n",
    "    ) \n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "    \n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "d-AtiqAYs_rH",
    "outputId": "32fd06b6-9d04-4391-fe3a-689a28734b72"
   },
   "outputs": [],
   "source": [
    "#Take a look on how the eval_task is inside square brackets and \n",
    "training_loop = train_model(model, train_task, [eval_task], 100, output_dir_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVMcsw2kjCX9"
   },
   "source": [
    "<a name=\"4-2\"></a>\n",
    "### 4.2 - Making a Prediction\n",
    "\n",
    "Now that we have trained our model, we can access it as `training_loop.model` object. We will actually use `training_loop.eval_model` \n",
    "\n",
    "Use the training data just to see how the prediction process works.  \n",
    "- Later, we will use validation data to evaluate your model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WAMgXWY4jCX-",
    "outputId": "7d732b79-6528-49cf-a78a-2f3ee4891681"
   },
   "outputs": [],
   "source": [
    "# Create a generator object\n",
    "tmp_train_generator = train_generator(\n",
    "                        16,\n",
    "                        train_pos,\n",
    "                        train_neg, \n",
    "                        Vocab, \n",
    "                        loop=True, \n",
    "                        shuffle = False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "5XoxD6u5jCX_",
    "outputId": "d857441c-0977-411f-a8de-2037820d8fa4"
   },
   "outputs": [],
   "source": [
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aJpFcyljCYB"
   },
   "source": [
    "To turn these probabilities into categories (negative or positive sentiment prediction), for each row:\n",
    "- Compare the probabilities in each column.\n",
    "- If column 1 has a value greater than column 0, classify that as a positive tweet.\n",
    "- Otherwise if column 1 is less than or equal to column 0, classify that example as a negative tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "6wJHv0TNjCYC",
    "outputId": "0367db61-6e29-44b5-be45-7534600e6931"
   },
   "outputs": [],
   "source": [
    "# turn probabilites into category predictions\n",
    "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TywSi02cjCYF"
   },
   "source": [
    "Notice that since we are making a prediction using a training batch, it's more likely that the model's predictions match the actual targets (labels).  \n",
    "- Every prediction that the tweet is positive is also matching the actual target of 1 (positive sentiment).\n",
    "- Similarly, all predictions that the sentiment is not positive matches the actual target of 0 (negative sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRRrgOHJgrhI"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Evaluation  \n",
    "\n",
    "<a name=\"5-1\"></a>\n",
    "### 5.1 - Computing the Accuracy on a Batch\n",
    "\n",
    "We will now write a function that evaluates the model on the validation set and returns the accuracy. \n",
    "- `preds` contains the predictions.\n",
    "    - Its dimensions are `(batch_size, output_dim)`.  `output_dim` is two in this case.  Column 0 contains the probability that the tweet belongs to class 0 (negative sentiment). Column 1 contains probability that it belongs to class 1 (positive sentiment).\n",
    "    - If the probability in column 1 is greater than the probability in column 0, then interpret this as the model's prediction that the example has label 1 (positive sentiment).  \n",
    "    - Otherwise, if the probabilities are equal or the probability in column 0 is higher, the model's prediction is 0 (negative sentiment).\n",
    "- `y` contains the actual labels.\n",
    "- `y_weights` contains the weights to give to predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hdfk3LEjCYL"
   },
   "source": [
    "<a name=\"5.1.1\"></a>\n",
    "### 5.1.1 - compute_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBqaN5f9grhJ"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(preds, y, y_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        preds: a tensor of shape (dim_batch, output_dim) \n",
    "        y: a tensor of shape (dim_batch,) with the true labels\n",
    "        y_weights: a n.ndarray with the a weight for each example\n",
    "    Output: \n",
    "        accuracy: a float between 0-1 \n",
    "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
    "        sum_weights (np.float32): Sum of the weights\n",
    "    \"\"\"\n",
    "    # Create an array of booleans, \n",
    "    # True if the probability of positive sentiment is greater than\n",
    "    # the probability of negative sentiment\n",
    "    # else False\n",
    "    is_pos = preds[:,1] > prediction[:,0]\n",
    "\n",
    "    # convert the array of booleans into an array of np.int32\n",
    "    is_pos.astype(np.int32)    \n",
    "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
    "    correct = tmp_is_positive == y\n",
    "\n",
    "    # Count the sum of the weights.\n",
    "    sum_weights = None\n",
    "    \n",
    "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
    "    correct_float = None\n",
    "    \n",
    "    # Multiply each prediction with its corresponding weight.\n",
    "    weighted_correct_float = None\n",
    "\n",
    "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
    "    # numerator.\n",
    "    weighted_num_correct = None\n",
    "\n",
    "    # Divide the number of weighted correct predictions by the sum of the\n",
    "    # weights.\n",
    "    accuracy = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1c7ZOeO0jCYN",
    "outputId": "a2a7414d-0168-4c55-b31f-bf263718c330"
   },
   "outputs": [],
   "source": [
    "# test your function\n",
    "tmp_val_generator = val_generator(64, val_pos\n",
    "                    , val_neg, Vocab, loop=True\n",
    "                    , shuffle = False)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_val_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
    "\n",
    "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
    "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2ep7nNejCYP"
   },
   "source": [
    "##### Expected output (Approximately)\n",
    "\n",
    "```\n",
    "Model's prediction accuracy on a single training batch is: 100.0%\n",
    "Weighted number of correct predictions 64.0; weighted number of total observations predicted 64\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function\n",
    "w1_unittest.test_compute_accuracy(compute_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqle69F1grhM"
   },
   "source": [
    "<a name=\"5-2\"></a>\n",
    "### 5.2 - Testing your Model on Validation Data\n",
    "\n",
    "Now you will write a test function to check your model's prediction accuracy on validation data. \n",
    "\n",
    "This program will take in a data generator and your model. \n",
    "- The generator allows you to get batches of data. You can use it with a `for` loop:\n",
    "\n",
    "```\n",
    "for batch in iterator: \n",
    "   # do something with that batch\n",
    "```\n",
    "\n",
    "`batch` has `3` elements:\n",
    "- the first element contains the inputs\n",
    "- the second element contains the targets\n",
    "- the third element contains the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zwYl_f9jCYP"
   },
   "source": [
    "<a name=\"5.2.1\"></a>\n",
    "### 5.2.1 - test_model\n",
    "\n",
    "**Instructions:** \n",
    "- Compute the accuracy over all the batches in the validation iterator. \n",
    "- Make use of `compute_accuracy`, which you recently implemented, and return the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKoTad4ggrhN"
   },
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "def test_model(generator, model, compute_accuracy=compute_accuracy):\n",
    "    '''\n",
    "    Input: \n",
    "        generator: an iterator instance that provides batches of inputs and targets\n",
    "        model: a model instance \n",
    "    Output: \n",
    "        accuracy: float corresponding to the accuracy\n",
    "    '''\n",
    "    \n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "        \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    for batch in generator: \n",
    "        \n",
    "        # Retrieve the inputs from the batch\n",
    "        inputs = None\n",
    "        \n",
    "        # Retrieve the targets (actual labels) from the batch\n",
    "        targets = None\n",
    "        \n",
    "        # Retrieve the example weight.\n",
    "        example_weight = None\n",
    "\n",
    "        # Make predictions using the inputs            \n",
    "        pred = None\n",
    "        \n",
    "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = None\n",
    "                \n",
    "        # Update the total number of correct predictions\n",
    "        # by adding the number of correct predictions from this batch\n",
    "        total_num_correct += None\n",
    "        \n",
    "        # Update the total number of predictions \n",
    "        # by adding the number of predictions made for the batch\n",
    "        total_num_pred += None\n",
    "\n",
    "    # Calculate accuracy over all examples\n",
    "    accuracy = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "1Rm_k21XgrhQ",
    "outputId": "65957ec4-d72b-4363-a5c2-20aa071e9005"
   },
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# testing the accuracy of your model: this takes around 20 seconds\n",
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16, val_pos\n",
    "                    , val_neg, Vocab, loop=False\n",
    "                    , shuffle = False), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esUJRMQPgrhS"
   },
   "source": [
    "##### Expected Output (Approximately)\n",
    "\n",
    "```CPP\n",
    "The accuracy of your model on the validation set is 0.9950\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_unittest.unittest_test_model(test_model, test_generator(16, val_pos , val_neg, Vocab, loop=False, shuffle = False), model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mct4P9QZgrhT"
   },
   "source": [
    "<a name=\"6\"></a>\n",
    "## 6 - Testing with your Own Input\n",
    "\n",
    "Finally you will test with your own input. You will see that deepnets are more powerful than the older methods you have used before. Although you go close to 100% accuracy on the first two assignments, the task was way easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUq5cw-xgrhU"
   },
   "outputs": [],
   "source": [
    "# this is used to predict on your own sentnece\n",
    "def predict(sentence):\n",
    "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    \n",
    "    # Batch size 1, add dimension for batch, to work with the model\n",
    "    inputs = inputs[None, :]  \n",
    "    \n",
    "    # predict with the model\n",
    "    preds_probs = model(inputs)\n",
    "    \n",
    "    # Turn probabilities into categories\n",
    "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
    "    \n",
    "    sentiment = \"negative\"\n",
    "    if preds == 1:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    return preds, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "3RJntC57grhX",
    "outputId": "01f92c2d-738f-424b-d2b0-6e1f6ade4fef"
   },
   "outputs": [],
   "source": [
    "# try a positive sentence\n",
    "sentence = \"It's such a nice day, I think I'll be taking Sid to Ramsgate for lunch and then to the beach maybe.\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "# try a negative sentence\n",
    "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZmGCheXjCYX"
   },
   "source": [
    "Notice that the model works well even for complex sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"7\"></a>\n",
    "## 7 - Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will visualize the word embeddings that were constructed for this sentiment analysis task. You can retrieve them by looking at the `model.weights` tuple (recall that the first layer of the model is the embedding layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the size of the embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the word embeddings, it is necessary to choose 2 directions to use as axes for the plot. You could use random directions or the first two eigenvectors from PCA. Here, you'll use scikit-learn to perform dimensionality reduction of the word embeddings using PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA #Import PCA from scikit-learn\n",
    "pca = PCA(n_components=2) #PCA with two dimensions\n",
    "\n",
    "emb_2dim = pca.fit_transform(embeddings) #Dimensionality reduction of the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything is ready to plot a selection of words in 2d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Selection of negative and positive words\n",
    "neg_words = ['worst', 'bad', 'hurt', 'sad', 'hate']\n",
    "pos_words = ['best', 'good', 'nice', 'better', 'love']\n",
    "\n",
    "#Index of each selected word\n",
    "neg_n = [Vocab[w] for w in neg_words]\n",
    "pos_n = [Vocab[w] for w in pos_words]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "#Scatter plot for negative words\n",
    "plt.scatter(emb_2dim[neg_n][:,0],emb_2dim[neg_n][:,1], color = 'r')\n",
    "for i, txt in enumerate(neg_words): \n",
    "    plt.annotate(txt, (emb_2dim[neg_n][i,0],emb_2dim[neg_n][i,1]))\n",
    "\n",
    "#Scatter plot for positive words\n",
    "plt.scatter(emb_2dim[pos_n][:,0],emb_2dim[pos_n][:,1], color = 'g')\n",
    "for i, txt in enumerate(pos_words): \n",
    "    plt.annotate(txt,(emb_2dim[pos_n][i,0],emb_2dim[pos_n][i,1]))\n",
    "\n",
    "plt.title('Word embeddings in 2d')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the word embeddings for this task seem to distinguish negative and positive meanings very well. However, clusters don't necessarily have similar words since you only trained the model to analyze overall sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNg0fAYIgrhd"
   },
   "source": [
    "### On Deep Nets\n",
    "\n",
    "Deep nets allow you to understand and capture dependencies that you would have not been able to capture with a simple linear regression, or logistic regression. \n",
    "- It also allows you to better use pre-trained embeddings for classification and tends to generalize better."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
