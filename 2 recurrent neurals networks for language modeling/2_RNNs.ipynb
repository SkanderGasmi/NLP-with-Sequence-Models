{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNNs, GRUs and the `scan` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will define the forward method for vanilla RNNs and GRUs. We will use the function `scan` to compute forward propagation for RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "from time import perf_counter\n",
    "\n",
    "from utils import sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Forward method for vanilla RNNs and GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we'll  implement the forward propagation for a vanilla RNN and the same method for a GRU. we'll use a set of random weights and variables with the following dimensions:\n",
    "\n",
    "- Embedding size (`emb`) : 128\n",
    "- Hidden state size (`h_dim`) : 16\n",
    "\n",
    "The weights `w_` and biases `b_` are initialized with dimensions (`h_dim`, `emb + h_dim`) and (`h_dim`, 1). We expect the hidden state `h_t` to be a column vector with size (`h_dim`,1) and the initial hidden state `h_0` is a vector of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed, \n",
    "random.seed(10)       \n",
    "# Embedding size (size of the input vector x_t at each time step )          \n",
    "emb = 128          \n",
    "# Number of variables in the sequences (number of t's )\n",
    "T = 256                        \n",
    "# Hidden state dimension\n",
    "h_dim = 16    \n",
    "# Initial hidden state                  \n",
    "h_0 = np.zeros((h_dim, 1))      \n",
    "# Random initialization of weights and biases with the gaussian normal distribution\n",
    "w1 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w2 = random.standard_normal((h_dim, emb+h_dim))\n",
    "w3 = random.standard_normal((h_dim, emb+h_dim))\n",
    "b1 = random.standard_normal((h_dim, 1))\n",
    "b2 = random.standard_normal((h_dim, 1))\n",
    "b3 = random.standard_normal((h_dim, 1))\n",
    "X = random.standard_normal((T, emb, 1))\n",
    "weights = [w1, w2, w3, b1, b2, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Forward method for vanilla RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanilla RNN cell is quite straight forward. Its most general structure is presented in the next figure: \n",
    "\n",
    "<img src=\"images/RNN.PNG\" width=\"400\"/>\n",
    "\n",
    "Here are the computations made in a vanilla RNN cell :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a vanilla RNN, the hidden state at time step $t$ ($h^{<t>}$) is computed as follows:\n",
    "\n",
    "$$\n",
    "h^{<t>} = g(W_{h}[h^{<t-1>}, x^{<t>}] + b_h)\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $h^{<t>}$ is the hidden state at time step $t$.\n",
    "- $x^{<t>}$ is the input at time step $t$.\n",
    "- $W_h$ is the weight matrix for the hidden state.\n",
    "- $[h^{<t-1>}, x^{<t>}]$ represents the concatenation of the previous hidden state $h^{<t-1>}$ and the current input $x^{<t>}$ vertically.\n",
    "- $b_h$ is the bias vector for the hidden state.\n",
    "- $g$ is the activation function.\n",
    "\n",
    "Additionally, the predicted output at time step $t$ ($\\hat{y}^{<t>}$) is computed as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y}^{<t>} = g(W_{yh}h^{<t>} + b_y)\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}^{<t>}$ is the predicted output at time step $t$.\n",
    "- $W_{yh}$ is the weight matrix for transforming the hidden state to the output.\n",
    "- $b_y$ is the bias vector for the output.\n",
    "\n",
    "The key operation here is the concatenation of $h^{<t-1>}$ and $x^{<t>}$ in equation (1), which combines the previous hidden state and the current input before applying the weight matrix $W_h$. In equation (2), the hidden state $h^{<t>}$ is used to compute the predicted output $\\hat{y}^{<t>}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation for a a single vanilla RNN cell\n",
    "def forward_vanilla_RNN(inputs, weights): \n",
    "    x, h_t = inputs\n",
    "\n",
    "    # weights.\n",
    "    wh, _, _, bh, _, _ = weights\n",
    "\n",
    "    # new hidden state\n",
    "    h_t = np.dot(wh, np.concatenate([h_t, x])) + bh\n",
    "    h_t = sigmoid(h_t)\n",
    "\n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We omitted the computation of $\\hat{y}^{<t>}$ for the sake of simplicity, to focus on the way that hidden states are updated here and in the GRU cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Forward method for GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GRU cell have more computations than the ones that vanilla RNNs have. You can see this visually in the following diagram:\n",
    "\n",
    "<img src=\"images/GRU.PNG\" width=\"400\"/>\n",
    "\n",
    "GRUs have relevance $\\Gamma_r$ and update $\\Gamma_u$ gates that control how the hidden state $h^{<t>}$ is updated on every time step. With these gates, GRUs are capable of keeping relevant information in the hidden state even for long sequences.(to avoid the vanishing gradient problem of the vanilla RNNs).\n",
    "\n",
    "The equations needed for the forward method in GRUs are provided below: \n",
    "\n",
    "\\begin{equation}\n",
    "\\Gamma_r=\\sigma{(W_r[h^{<t-1>}, x^{<t>}]+b_r)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\Gamma_u=\\sigma{(W_u[h^{<t-1>}, x^{<t>}]+b_u)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "c^{<t>}=\\tanh{(W_h[\\Gamma_r*h^{<t-1>},x^{<t>}]+b_h)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h^{<t>}=\\Gamma_u*c^{<t>}+(1-\\Gamma_u)*h^{<t-1>}\n",
    "\\end{equation}\n",
    "\n",
    "Now we will implement the forward method for a GRU cell by computing the update `u` and relevance `r` gates, and the candidate hidden state `c`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation for a single GRU cell\n",
    "def forward_GRU(inputs, weights): \n",
    "    x, h_t = inputs\n",
    "\n",
    "    # weights.\n",
    "    wu, wr, wc, bu, br, bc = weights\n",
    "\n",
    "    # Update gate\n",
    "    u = np.dot(wu, np.concatenate([h_t, x])) + bu\n",
    "    u = sigmoid(u)\n",
    "    \n",
    "    # Relevance gate\n",
    "    r = np.dot(wr, np.concatenate([h_t, x])) + br\n",
    "    r = sigmoid(r)\n",
    "    \n",
    "    # Candidate hidden state \n",
    "    c = np.dot(wc, np.concatenate([r * h_t, x])) + bc\n",
    "    c = np.tanh(c)\n",
    "    \n",
    "    # New Hidden state h_t\n",
    "    h_t = u* c + (1 - u)* h_t\n",
    "    return h_t, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implementation of the `scan` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scan` function is used for forward propagation in RNNs. It takes as inputs:\n",
    "\n",
    "- `fn` : the function to be called recurrently (i.e. `forward_GRU`)\n",
    "- `elems` : the list of inputs for each time step (`X`)\n",
    "- `weights` : the parameters needed to compute `fn`\n",
    "- `h_0` : the initial hidden state\n",
    "\n",
    "`scan` goes through all the elements `x` in `elems`, calls the function `fn` with arguments ([`x`, `h_t`],`weights`), stores the computed hidden state `h_t` and appends the result to a list `y_hats`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation for RNNs\n",
    "def scan(fn, elems, weights, h_0=None): \n",
    "    h_t = h_0\n",
    "    y_hats = []\n",
    "    for x in elems:\n",
    "        y, h_t = fn([x, h_t], weights)\n",
    "        y_hats.append(y)\n",
    "    return y_hats, h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Comparison between vanilla RNNs and GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUs perform more computations than vanilla RNNs, as they have 3 times more parameters. In the next two cells, we compute forward propagation for a sequence with 256 time steps (`T`) for an RNN and a GRU with the same hidden state `h_t` size (`h_dim`=16).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 2.50ms to run the forward method for the vanilla RNN.\n"
     ]
    }
   ],
   "source": [
    "# vanilla RNNs\n",
    "tic = perf_counter()\n",
    "y_hats, h_T = scan(forward_vanilla_RNN, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "vanilla_RNN_time=(toc-tic)*1000\n",
    "print (f\"It took {vanilla_RNN_time:.2f}ms to run the forward method for the vanilla RNN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 6.70ms to run the forward method for the GRU.\n"
     ]
    }
   ],
   "source": [
    "# GRUs\n",
    "tic = perf_counter()\n",
    "y_hats, h_T = scan(forward_GRU, X, weights, h_0)\n",
    "toc = perf_counter()\n",
    "GRU_time=(toc-tic)*1000\n",
    "print (f\"It took {GRU_time:.2f}ms to run the forward method for the GRU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUs take more time to compute. This means that training and prediction would take more time for a GRU than for a vanilla RNN. However, GRUs allows us to propagate relevant information even for long sequences, so when selecting an architecture for NLP we should assess the tradeoff between computational time and performance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
